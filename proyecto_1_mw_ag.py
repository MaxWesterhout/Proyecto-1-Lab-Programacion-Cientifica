# -*- coding: utf-8 -*-
"""Proyecto_1_mw_ag.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11v26ohoy_SDqMuVRYxA8Axv7-2a82hEM

![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)

# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**

**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**

### Cuerpo Docente:

- Profesor: Ignacio Meza, Gabriel Iturra
- Auxiliar: Sebasti√°n Tinoco
- Ayudante: Arturo Lazcano, Angelo Mu√±oz

*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*

### Equipo:

- Alvaro Gallardo
- Maximiliano Westerhout


### Link de repositorio de GitHub: `\<https://github.com/MaxWesterhout/Proyecto-1-Lab-Programacion-Cientifica.git>`

Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023.

----

## Reglas

- **Grupos de 2 personas.**
- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.
- Estrictamente prohibida la copia.
- Pueden usar cualquier material del curso que estimen conveniente.

<div style="text-align: center;">
    <img src="https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg" alt="Descripci√≥n de la imagen">
</div>

En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.

El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:
- Caracterizaci√≥n autom√°tica de los datos
- La soluci√≥n debe ser compatible con cualquier dataset
- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os

## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)

Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:

1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.

2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:
    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.
    - Reportar el tipo de variable
    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable
    - Reportar el n√∫mero y/o porcentaje de valores nulos
    - Si la variables es num√©rica:
        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers
        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100
   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)
   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.

3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:
    - Crear la carpeta `EDA_fecha/plots`
    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.
    - Para las variables num√©ricas:
        - Genere un gr√°fico de distribuci√≥n de densidad
        - Grafique la correlaci√≥n entre las variables
    - Para las variables categ√≥ricas:
        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)
        - Grafique el coeficiente V de Cramer entre las variables
    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s
    
4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:
    - Crear la carpeta `EDA_fecha/clean_data`
    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.
    - Drop de valores duplicados
    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:
        - Drop de valores nulos
        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n
        - Funcionalidad para escoger entre una t√©cnica y la otra.
    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.
        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?
        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.
    - Deber√≠an usar `FunctionTransformer`.
    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`

5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:
    - Crear la carpeta `EDA_fecha/scale`
    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:
        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)
        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`
        - Asuma que no existen datos ordinales en su dataset
    - Guardar todo este procesamiento en un `ColumnTransformer`.
    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`

6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:
    - Crear la carpeta `EDA_fecha/clusters`
    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.
    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).
    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering.
    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.
    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).
    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.
    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.

7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:

    - Crear la carpeta `EDA_fecha/anomalies`
    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.
    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:
        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a.
        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.
        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).
        - Su m√©todo debe recibir el algoritmo como argumento de entrada
        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as
    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.

8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.

9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.

Algunas consideraciones generales:
- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset.
- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.
- Recuerden documentar cada una de las funcionalidades que implementen.
- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**

### Mega hiper profiler
"""

!pip install umap-learn

#Importaciones
from numpy.core import numeric
import pandas as pd
import umap
import scipy. stats  as stats
import seaborn as sns
from datetime import datetime
import os
import shutil
import re
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from scipy.stats import chi2_contingency
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from scipy.stats import zscore
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN

#Funciones auxiliares

#-----------------------------------------------------------------------------
#MinMax() a utilizar en la funcion scale
# Aplica logaritmo ademas del MinMax()
class MinMax(BaseEstimator, TransformerMixin):
    def fit(self, X):
        X_log = np.log(X+1)
        self.X_log = X_log
        self.min_vals = X_log.min()
        self.max_vals = X_log.max()
        return self

    def transform(self, X):
        # Aplicamos la transformaci√≥n Min-Max a cada columna
        X_transformed = (self.X_log - self.min_vals) / (self.max_vals - self.min_vals)

        return X_transformed
#-----------------------------------------------------------------------------
#Funciones que se utilizan en clean data

#Eliminacion valores duplicados
def drop_duplicates(datos_var):
  datos_var = datos_var.drop_duplicates()
  return datos_var

# Tecnicas valores nulos / son 3
# drop, imputar_ceros y imputar_media
def valores_nulos(datos_var, funcionalidad=0):
    # Drop valores nulos

    if funcionalidad == 0 or funcionalidad == 'drop':
        datos_var = datos_var.dropna()
        return datos_var

    if funcionalidad == 'imputar_ceros':
        # Imputar datos con ceros en la columna
        datos_var = datos_var.fillna(0)
        return datos_var

    # Seleccionar solo las columnas num√©ricas
    columnas_numericas = datos_var.select_dtypes(include=['float', 'int'])

    if funcionalidad == 'imputar_media':
        # Imputar la media en las columnas num√©ricas
        media_columnas = columnas_numericas.mean()
        datos_var[columnas_numericas.columns] = columnas_numericas.fillna(media_columnas)
        return datos_var

    return datos_var

#Funcion para separar datos no atomicos
def separate_non_atomic(data, eleccion = [] , separator=[':', "(", "?", '*',',']):
    #Vectores y parametros extras
    no_atomicas = []
    elecciones_correctas = []
    k = 0

    #---------------------------------------------------------------------------------------
    #Este for lo unico que hace es que verifica que elementos de eleccion estan presentes en
    # el dataframe y cuales no, simlemente para levantar errores y mensajes personalizados para el usuario
    for i in range(len(eleccion)):
      if eleccion[i] in(data.columns):
        k = k+1
        elecciones_correctas.append(eleccion[i])

      if eleccion[i] not in(data.columns):
        mensaje='Warning: La variable '+ str(eleccion[i]) + ' no pertenece al DataFrame'
        print('*'*80)
        print(mensaje)
        print('*'*80)

      if i == len(eleccion)-1 and k == 0:
        raise Exception('Ninguna columna fue separada')

    #Este if setea que si no se elije ninguna opcion se busque en todas las columnas
    if len(eleccion) == 0:
      eleccion= data.columns
    #---------------------------------------------------------------------------------------
    #Este for busca los separadores presentes en todas las columnas y selecciona cuales columnas
    #son no atomicas, se debe tener presente que esta funcion tambien hace esto de forma automatica
    for var in data.columns:
      if not pd.api.types.is_numeric_dtype(data[var]):
          for sep in separator:
              # Escapa los caracteres especiales en el separador con re.escape
              sep_escaped = re.escape(sep)
              if data[var].str.contains(sep_escaped).any():
                  no_atomicas.append(var)
                  break
    #---------------------------------------------------------------------------------------
    #Este for imprime un mensaje que indica al usuario si las variables que coloco son efecto no-atomicas
    for var_no_atomicas in elecciones_correctas:
      if var_no_atomicas not in(no_atomicas):
        mensaje='Warning: La variable '+ str(var_no_atomicas) + ' es una columna atomica (no contiene separadores)'
        print('*'*80)
        print(mensaje)
        print('*'*80)
    #---------------------------------------------------------------------------------------
    #Este for separa las columnas verificando que sean las eligidas y que a la vez sean no-atomicas
    new_column_names = []
    for var in no_atomicas:
      if var in eleccion:
        for sep in separator:
            data[var] = data[var].str.replace(sep, '-',regex=True)

        n = data[var].str.count('-').max()

        split_columns = data[var].str.split('-', expand=True)
        new_column_names = [var + '_' + str(i) for i in range(n + 1)]
        split_columns.columns = new_column_names

        for col in new_column_names:
            data[col] = split_columns[col]  # Almacena la informacion en las nuevas columnas

        data.drop(var, axis=1, inplace=True)


    #Agrega Nan en las que esten vacias
    data.replace('nan', pd.NA, inplace=True)  # Reemplaza 'nan' por valores nulos de Pandas

    #OJO QUE LAS NUEVAS COLUMNAS SERIAN OBJECT, POR ESO DEBEMOS APLICAR UN FILTRADO NUEVAMENTE PARA CONVERTIR LAS COLUMNAS
    # Intentar convertir la columna 'Column1' a 'int'
    if len(new_column_names) !=0:
      columnas_previas = data[new_column_names]

      # Verificar si hay valores NaN en la columna
      for j in new_column_names:
        data[j] = pd.to_numeric(data[j], errors='coerce', downcast='float')

        #Si toda la nueva columna es NaN no era flot o int, por ello vuelve a su valor inicial de object
        if data[j].isnull().all() == True:
          data[j] = columnas_previas[j]

    #Se retorna la nueva data
    return data
#----------------------------------------------------------------------------------------

##############################################################################
#Clase Profiler
#Cabe destacar que la clase utiliza la funcion datetime.now(), esta libreria esta seteada
#con el tiempo de otro pais, con aproximadamente 4 horas de diferencias por ello si se
#ejecuta el codigo a las 9 aparecera como si fuera el dia siguiente.

#Se almacena el path y la informacion del dataframe
class Profiler():

    def __init__(self, input_df):
      if isinstance(input_df,pd.DataFrame):
        self.df = input_df
        self.variables = input_df.columns

        #Almacenamos variables de la fecha con datetime
        now = datetime.now()
        mes = now.strftime("%m")
        dia = now.strftime("%d")
        a√±o = now.strftime("%y")
        #Nombre a utilizar en la carpeta EDA_fecha
        fecha = dia + "-" + mes + "-" + a√±o
        nombre_carpeta = "EDA_" + fecha
        #Creacion carpeta EDA_fecha
        path = "/content/" + nombre_carpeta
        self.path = path

        if os.path.exists(path) == True:
          #La carpeta ya existe
          pass
        else:
          os.mkdir(path)
      else:
          raise Exception("Coloque un dataframe correcto")

###############################################################################
# Tipos de anomalias: 1) Numero de valores nulos superiores a 40%
#                     2) Numero de valores negativos superiores a 40%
#                     3) Numero de valores outlayers superiores a 10%

    def summarize(self,variables):
      datos = self.df

      directorio = str(self.path) + '/' + 'summary'
      mensaje = ''

      for j in range(len(variables)):
        mensaje = mensaje + str(variables[j])
        if j + 1 < len(variables):
          mensaje = mensaje + ','

      mensaje_inicial = "Summary de las variables: " + mensaje

      var_correctas = []
      var_data_frame = self.df.columns.to_list()

      #------------------------------------------------------------------------
      for i in variables:
        if i not in(var_data_frame):
          pass
        else:
          var_correctas.append(i)

      # Seleccionar caracter√≠sticas num√©ricas y categ√≥ricas
      variables_numericas_f = datos[var_correctas].select_dtypes(include=['number'])
      variables_categoricas_f = datos[var_correctas].select_dtypes(exclude=['number'])

      variables_numericas = variables_numericas_f.columns.to_list()
      variables_categoricas = variables_categoricas_f.columns.to_list()
      #------------------------------------------------------------------------

      with open(directorio, 'a') as f:
        f.write('-'*50)
        f.write('\n')
        f.write(mensaje_inicial)
        f.write('\n')
        f.write('-'*50)
        f.write('\n')

      for i in variables:
        if i in datos.columns:
          #Mensajes para escribir
          m = []
          columna = datos[i]
          m.append("*"*50)
          m.append("summarize variable "+ str(i))
          m.append("*"*50)

          m.append("Informacion de la variable:")
          #Reportar el tipo de variable
          m.append("Tipo de datos variable: " + str(columna.dtypes))
          m.append("Numero de valores unicos: " + str(columna.nunique()))
          numeros_nulos = columna.isna().sum()

          m.append("Numero de valores nulos: " + str(columna.isna().sum()))

          if i in variables_numericas:
            #Obtencion valores iguales a ceros y negativos
            numero_ceros = (columna == 0).sum()
            numero_negativos = (columna < 0).sum()

            #Obtencion outlayers (por medio de los cuartiles)
            Q1 = columna.quantile(0.25)
            Q2 = columna.quantile(0.50)
            Q3 = columna.quantile(0.75)
            Q4 = columna.quantile(1)
            IQR = Q3 - Q1 #Rango intercuartil
            numero_outlayers = ((columna < (Q1 - 1.5 * IQR)) | (columna > (Q3 + 1.5 * IQR))).sum()

            #Estadistica descriptiva
            maximo = columna.max()
            minimo = columna.min()

            #Print de la informacion
            m.append('-'*50)
            m.append("Informacion numerica de la variable:")
            m.append("Numero de valores iguales a cero: " + str(numero_ceros))
            m.append("Numero de valores negativos: " + str(numero_negativos))
            m.append("Numero de outlayers: " + str(numero_outlayers))
            m.append("Maximo valor: " + str(maximo))
            m.append("Minimo valor: " + str(minimo))
            m.append("Percentil 25: " + str(Q1))
            m.append("Percentil 50: " + str(Q2))
            m.append("Percentil 75: " + str(Q3))
            m.append("Percentil 100: " + str(Q4))

            #LEVANTAR ALERTAS
            #Primera alerta: 40% o mas de valores nulos
            if numeros_nulos >=  len(columna)*0.4:
              m.append("~"*50)
              m.append('ALERTA: La variable esta compuesta en su mayoria por valores nulos')
              m.append("~"*50)

            if numero_negativos >= len(columna)*0.4:
              m.append("~"*50)
              m.append('ALERTA: atencion tu variable tiene muchos valores negativos, esto es correcto?')
              m.append("~"*50)

            if numero_outlayers >= len(columna)*0.1:
              m.append("~"*50)
              m.append('ALERTA: tu variable tiene mas del 10% de outlayers')
              m.append("~"*50)

            #Almacenamos valores
            with open(directorio, 'a') as f:
              for i in m:
               f.write(i)
               f.write('\n')

          else:
            m.append('La variable no es numerica')

            if numeros_nulos >=  len(columna)*0.4:
              m.append("~"*50)
              m.append('ALERTA: La variable esta compuesta en su mayoria por valores nulos')
              m.append("~"*50)

            #Almacenamos valores
            directorio = str(self.path) + '/' + 'summary'
            with open(directorio, 'a') as f:
              for i in m:
               f.write(i)
               f.write('\n')
        else:
          with open(directorio, 'a') as f:
            f.write("*"*50)
            f.write('\n')
            f.write("La variable " + str(i) + " no pertenece al dataframe en cuestion")
            f.write('\n')
            f.write("*"*50)
            f.write('\n')
###########################################################################################
# top_n debe ser una lista, que contenga nombres de las columnas del dataframe

    def plot_vars(self,variables,top_n = []):
        #Creacion carpeta plots
        path = str(self.path) + '/plots'

        if os.path.exists(path) == False:
          os.mkdir(path)

        #Variables a utilizar
        datos = self.df
        top_n_correcto = []
        var_data_frame = self.df.columns.to_list()
        var_correctas = []


        for i in variables:
          if i not in(var_data_frame):
            print("*"*50)
            print("La variable",i,"no pertenece al dataframe en cuestion")
            print("*"*50)
          else:
            var_correctas.append(i)

        # Seleccionar caracter√≠sticas num√©ricas y categ√≥ricas
        variables_numericas_f = datos[var_correctas].select_dtypes(include=['number'])
        variables_categoricas_f = datos[var_correctas].select_dtypes(exclude=['number'])

        variables_numericas = variables_numericas_f.columns.to_list()
        variables_categoricas = variables_categoricas_f.columns.to_list()

        #Verificacion top_n categorias correctas
        for i in top_n:
          if i in(datos.columns):
            top_n_correcto.append(i)
          else:
            print("*"*50)
            print("La variable",i,"no pertenece al dataframe en cuestion (top_n categorias)")
            print("*"*50)

        #------------------------------------------------------------------------------------
        #Plots variables numericas
        if len(variables_numericas) != 0:
          #Grafico densidades de cada variable
          for i in variables_numericas:
              columna = datos[i]
              columna.plot.density(color='red')
              titulo = 'Density plot de ' + str(i)
              nombre = str(i) + ".pdf"
              plt.title(titulo)
              directorio_num = path +'/'+ nombre
              plt.savefig(directorio_num, format="pdf")
              plt.show()

          #Grafico correlaciones
          if len(variables_numericas)>1:
            df = datos[variables_numericas]
            corr = df.corr()
            corr.style.background_gradient(cmap='coolwarm')

            # Crear la visualizaci√≥n de la matriz de correlaci√≥n con valores de correlaci√≥n
            plt.figure(figsize=(10, 8))
            cax = plt.matshow(corr, cmap='coolwarm', interpolation='none', aspect='auto')
            plt.colorbar(cax)

            plt.title('Matriz de Correlaci√≥n', y=-0.1)
            plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
            plt.yticks(range(len(corr.columns)), corr.columns)

            for i in range(len(corr.columns)):
                for j in range(len(corr.columns)):
                    plt.text(j, i, f'{corr.iloc[i, j]:.2f}', ha='center', va='center', color='w')

            nombre = 'Matriz_de_correlacion_variables_numericas.pdf'
            directorio_num = path +'/'+ nombre
            plt.savefig(directorio_num, format="pdf")
            plt.show()
        #------------------------------------------------------------------------------------
        #Plots variables categoricas

        if len(variables_categoricas) != 0:
          #dos casos: 1) no se coloca top_n, 2) se coloca

          #Caso no se categorizan las variables categoricas
          if len(top_n_correcto) == 0:
            for i in variables_categoricas:
              datos[i] = datos[i].astype(str)
              columna = datos[i]
              columna.hist(grid=False)
              titulo = 'Histograma de ' + str(i)
              nombre = str(i) + ".pdf"
              plt.title(titulo)
              directorio_num = path +'/'+ nombre
              plt.savefig(directorio_num, format="pdf")
              plt.xticks(rotation=90)
              plt.show()

            #Coeficiente V crameR
            if len(variables_categoricas) > 0:
                le = LabelEncoder()
                # Crear una matriz para almacenar los valores de Cramer's V
                cramers_v_matrix = np.zeros((len(variables_categoricas), len(variables_categoricas)))

                # Recorrer todas las combinaciones de pares de variables categ√≥ricas
                for i in range(len(variables_categoricas)):
                    for j in range(len(variables_categoricas)):
                        if i != j:
                            var1 = variables_categoricas[i]
                            var2 = variables_categoricas[j]
                            encoded_var_data1 = le.fit_transform(datos[var1])
                            encoded_var_data2 = le.fit_transform(datos[var2])
                            confusion_matrix = pd.crosstab(encoded_var_data1, encoded_var_data2)
                            chi2 = chi2_contingency(confusion_matrix)[0]
                            n = len(datos)
                            min_dim = min(confusion_matrix.shape) - 1
                            if min_dim != 0:
                                cramers_v = np.sqrt(chi2 / (n * min_dim))
                            else:
                                cramers_v = 0
                            cramers_v_matrix[i][j] = cramers_v
                        else:
                          cramers_v_matrix[i][j] = 1

                #Se grafica la matriz
                plt.figure(figsize=(10, 8))
                sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', fmt=".2f",
                            xticklabels=variables_categoricas, yticklabels=variables_categoricas)
                plt.title("Cramer's V Matrix")
                plot_filename = os.path.join(path, "cramers_v_matrix.pdf")
                plt.savefig(plot_filename, format="pdf")
                plt.show()

          else:
            #Caso se categorizan
            for i in top_n_correcto:
              datos[i] = datos[i].astype(str)
              columna = datos[i]
              columna.hist(grid=False)
              titulo = 'Histograma de ' + str(i)
              nombre = str(i) + ".pdf"
              plt.title(titulo)
              directorio_num = path +'/'+ nombre
              plt.savefig(directorio_num, format="pdf")
              plt.xticks(rotation=90)
              plt.show()

            #Coeficiente V cramer
            if len(top_n_correcto) > 0:
                le = LabelEncoder()
                # Crear una matriz para almacenar los valores de Cramer's V
                cramers_v_matrix = np.zeros((len(top_n_correcto), len(top_n_correcto)))
                # Recorrer todas las combinaciones de pares de variables categ√≥ricas
                for i in range(len(top_n_correcto)):
                    for j in range(len(top_n_correcto)):
                        if i != j:
                            var1 = top_n_correcto[i]
                            var2 = top_n_correcto[j]
                            encoded_var_data1 = le.fit_transform(datos[var1])
                            encoded_var_data2 = le.fit_transform(datos[var2])
                            confusion_matrix = pd.crosstab(encoded_var_data1, encoded_var_data2)
                            chi2 = chi2_contingency(confusion_matrix)[0]
                            n = len(datos)
                            min_dim = min(confusion_matrix.shape) - 1
                            if min_dim != 0:
                                cramers_v = np.sqrt(chi2 / (n * min_dim))
                            else:
                                cramers_v = 0

                            cramers_v_matrix[i][j] = cramers_v
                        else:
                          cramers_v_matrix[i][j] = 1

                #Se grafica la matriz
                plt.figure(figsize=(10, 8))
                sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', fmt=".2f",
                            xticklabels=top_n_correcto, yticklabels=top_n_correcto)
                plt.title("Cramer's V Matrix")
                plot_filename = os.path.join(path, "cramers_v_matrix.pdf")
                plt.savefig(plot_filename, format="pdf")
                plt.show()

###########################################################################################
#COMPLETA, LOS METODOS SON: drop, imputar_media y imputar_ceros

    def clean_data(self,variables, metodo = 0, eleccion_input = []):
        #Creacion carpeta clean_data
        path = str(self.path) + '/clean_data'

        if os.path.exists(path) == False:
          os.mkdir(path)

        #sensibilidad de la variable 'variables'
        if isinstance(variables, list) == False:
          vector_correcto = [variables]
          variables = vector_correcto

        #dataframe a utilizar
        datos_var = self.df[variables].copy()

        #Datos atomicos
        # Crea un FunctionTransformer
        nulos = FunctionTransformer(func=lambda x: valores_nulos(x, funcionalidad = metodo), validate=False)
        duplicates = FunctionTransformer(func=drop_duplicates,validate=False)
        separacion = FunctionTransformer(func=lambda x: separate_non_atomic(x, eleccion = eleccion_input), validate=False)

        # Crea un pipeline de transformaci√≥n que incluye el ColumnTransformer
        # Orden: Seperacion - Nulos - Duplicates

        pipeline = Pipeline([
            ('separacion', separacion),
            ('nulos', nulos),
            ('duplicates', duplicates)
        ])


        # Aplicamos el pipeline a los datos
        data_reduced = pipeline.fit_transform(datos_var)

        data_reduced = data_reduced.reset_index(drop=True)

        #Creacion archivo csv con los datos
        path_csv = path + '/' + 'data.csv'
        data_reduced.to_csv(path_csv,index=False)


        return data_reduced
############################################################################################
# Tecnicas de escalamiento adicionales: StandardScaler y RobustScaler

    def scale(self,variables, tecnicas=0):
        #Creacion carpeta scale
        path = str(self.path) + '/scale'
        data = self.df


        if os.path.exists(path) == False:
          os.mkdir(path)


        # Seleccionar caracter√≠sticas num√©ricas y categ√≥ricas
        numeric_features = data[variables].select_dtypes(include=['number'])
        categorical_features = data[variables].select_dtypes(exclude=['number'])

        if (numeric_features < 0).any().any():
          print('*'*80)
          print('ALERTA: Sus variables presentan valores negativos, logaritmo entregara NaN')
          print('*'*80)

        #Para las variables categoricas aplicamos one_hot_encoder
        #ohe = OneHotEncoder()
        #cod = ohe.fit_transform(categorical_features)
        #categoric_df = pd.DataFrame(cod.toarray(), columns=ohe.categories_)
        categoric_df = categorical_features

        if tecnicas == 0:
          # Definir el escalador logar√≠tmico y MinMaxScaler

          log_scaler = ColumnTransformer([("scaler", MinMax(),slice(0,None))], remainder="passthrough")

          # Obtener los nombres de las columnas originales
          orig_column_names = numeric_features.columns.tolist()

          # Obtener los nuevos nombres de las columnas despu√©s de aplicar las t√©cnicas
          new_column_names = [f"{col}_log_min_max" for col in orig_column_names]  # Agrega '_log' al final de los nombres

          # Aplicar el ColumnTransformer a los datos
          scaled_data = log_scaler.fit_transform(numeric_features)

          # Crear un DataFrame con los datos transformados
          scaled_df = pd.DataFrame(scaled_data, columns=new_column_names)

          # A√±adir las columnas categ√≥ricas al DataFrame
          scaled_df = pd.concat([scaled_df, categoric_df], axis=1)

          #Creacion archivo csv con los datos
          path_csv = path + '/' + 'scaled_features.csv'
          scaled_df.to_csv(path_csv,index=False)
          return scaled_df

        else:
          #Standarizacion de standard scaler
          if tecnicas == 'StandardScaler':
            standard_scaler = StandardScaler()
            estandarized_df = standard_scaler.fit_transform(numeric_features)
            estandarized_df = pd.DataFrame(estandarized_df)

            #Alerta
            if (estandarized_df < 0).any().any():
              print('*'*80)
              print('ALERTA: Su estandirazicion presento valores negativos, logaritmo entregara NaN se dropean dichos datos')
              print('*'*80)

            #Mismo proceso log y minMax pero con data standarizada
            log_scaler = ColumnTransformer([("scaler", MinMax(),slice(0,None))], remainder="passthrough")

            # Obtener los nombres de las columnas originales
            orig_column_names = numeric_features.columns.tolist()

            # Obtener los nuevos nombres de las columnas despu√©s de aplicar las t√©cnicas
            new_column_names = [f"{col}_log_min_max" for col in orig_column_names]  # Agrega '_log' al final de los nombres

            # Aplicar el ColumnTransformer a los datos
            scaled_data = log_scaler.fit_transform(estandarized_df)

            # Crear un DataFrame con los datos transformados
            scaled_df = pd.DataFrame(scaled_data, columns=new_column_names)

            scaled_df = scaled_df.dropna().reset_index()
            scaled_df = scaled_df.drop(['index'],axis=1)
            tama√±o_antiguo = len(scaled_df)


            # A√±adir las columnas categ√≥ricas al DataFrame
            scaled_df = pd.concat([scaled_df, categoric_df], axis=1,ignore_index=True)
            scaled_df = scaled_df[:tama√±o_antiguo]

            #Creacion archivo csv con los datos
            path_csv = path + '/' + 'scaled_features.csv'
            scaled_df.to_csv(path_csv,index=False)

            if len(scaled_df) == 0:
              raise Exception('Este conjunto de datos no aplica para este escalamiento (0 datos)')

            return scaled_df

          #Robustez
          if tecnicas == 'RobustScaler':
            robust_scaler = RobustScaler()
            robust_scaled_df = robust_scaler.fit_transform(numeric_features)
            robust_scaled_df = pd.DataFrame(robust_scaled_df)

            if (robust_scaled_df < 0).any().any():
              print('*'*80)
              print('ALERTA: Su estandirazicion presento valores negativos, logaritmo entregara NaN se dropean dichos datos')
              print('*'*80)

            #Mismo proceso log y minMax pero con data standarizada
            log_scaler = ColumnTransformer([("scaler", MinMax(),slice(0,None))], remainder="passthrough")

            # Obtener los nombres de las columnas originales
            orig_column_names = numeric_features.columns.tolist()

            # Obtener los nuevos nombres de las columnas despu√©s de aplicar las t√©cnicas
            new_column_names = [f"{col}_log_min_max" for col in orig_column_names]  # Agrega '_log' al final de los nombres

            # Aplicar el ColumnTransformer a los datos
            scaled_data = log_scaler.fit_transform(robust_scaled_df)

            # Crear un DataFrame con los datos transformados
            scaled_df = pd.DataFrame(scaled_data, columns=new_column_names)

            scaled_df = scaled_df.dropna().reset_index()
            scaled_df = scaled_df.drop(['index'],axis=1)
            tama√±o_antiguo = len(scaled_df)


            # A√±adir las columnas categ√≥ricas al DataFrame
            scaled_df = pd.concat([scaled_df, categoric_df], axis=1,ignore_index=True)
            scaled_df = scaled_df[:tama√±o_antiguo]

            #Creacion archivo csv con los datos
            path_csv = path + '/' + 'scaled_features.csv'
            scaled_df.to_csv(path_csv,index=False)

            if len(scaled_df) == 0:
              raise Exception('Este conjunto de datos no aplica para este escalamiento (0 datos)')

            return scaled_df

          else:
            raise Exception('Ese tipo de tecnica no esta contemplada')

############################################################################################
#Genera clusters

    def make_clusters(self,tecnica_limpieza = 0, tecnica_escalar = 0,variables_separar=[],k=0):
        #Creacion carpeta clusters
        path = str(self.path) + '/clusters'

        if os.path.exists(path) == False:
          os.mkdir(path)

        data = self.df
        variables = data.columns
        var_text = variables.to_list()

        #Caso se ejecute make_clusters sin haber limpiado la data ni escalado (se deben realizar)
        if k == 0:
          #----------------------------------------------------------------------
          #Se aplica 4
          var_text = variables.values.astype(str).tolist()
          clean_data_method = self.clean_data(var_text, metodo = tecnica_limpieza, eleccion_input = variables_separar).reset_index()
          self.df = clean_data_method.drop(['index'], axis=1)

          #Se reescribe la data
          data = self.df
          variables = data.columns
          var_text = variables.to_list()

        #----------------------------------------------------------------------
        #Se aplica 5
        scale_method = self.scale(var_text, tecnicas= tecnica_escalar)
        #Se reescribe la data
        self.df = scale_method
        #----------------------------------------------------------------------
        data = self.df
        variables = data.columns

        # Seleccionar caracter√≠sticas num√©ricas y categ√≥ricas
        numeric_features = data[variables].select_dtypes(include=['number'])
        categorical_features = data[variables].select_dtypes(exclude=['number'])

        Largo_iteraciones = len(data) - 1
        if Largo_iteraciones > 20:
          Largo_iteraciones = 19

        results = []


        for k in range(1, Largo_iteraciones):
          # Paso 4: Aplicar K-Means con diferentes valores de k
          kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)

          # Creamos el pipeline
          pipeline = Pipeline([
              ('kmeans', kmeans)
          ])

          # Aplicamos el pipeline a los datos
          pipeline.fit_transform(numeric_features)

          # Guardamos la inercia para este valor de k
          results.append(pipeline.named_steps['kmeans'].inertia_)

        # Graficamos la inercia en funci√≥n de k
        plt.figure(figsize=(8, 6))
        plt.plot(range(1, Largo_iteraciones), results, marker='o')
        plt.title("M√©todo del Codo para Determinar el N√∫mero √ìptimo de Clusters")
        plt.xlabel("N√∫mero de Clusters (k)")
        plt.ylabel("Inercia")

        nombre = 'Grafico del codo.pdf'
        directorio_num = path +'/'+ nombre
        plt.savefig(directorio_num, format="pdf")
        plt.show()

        if len(numeric_features.columns) > 1:

          print('*'*80)
          print('Inserte el numero de clusters que quiere utilizar:')
          n_clusters_input = input('Escribe aqui: ')
          print('*'*80)

          if len(n_clusters_input) == 0:
            raise Exception('Debe colocar un numero de clusters a utilizar')

          n_clusters_input = int(n_clusters_input)

          kmeans = KMeans(n_clusters=n_clusters_input, random_state=42)

          clusters = kmeans.fit_predict(numeric_features)

          # Agregar las etiquetas de los clusters al DataFrame original
          data['Cluster'] = kmeans.labels_

          # Realizar PCA para reducir la dimensionalidad a 2 componentes principales
          pca = PCA(n_components=2)
          pca_result = pca.fit_transform(numeric_features)

          pca_df = pd.DataFrame(pca_result, columns=['PCA1', 'PCA2'])

          # Concatenar el DataFrame PCA con el DataFrame original
          result_df = pd.concat([data, pca_df], axis=1)

          # Graficar los clusters en el espacio PCA
          plt.figure(figsize=(8, 6))
          plt.scatter(result_df['PCA1'], result_df['PCA2'], c=result_df['Cluster'], cmap='viridis')
          plt.title("Resultados de Clustering en Espacio PCA")
          plt.xlabel("PCA1")
          plt.ylabel("PCA2")

          nombre = 'Clusterizacion de las variables numericas.pdf'
          directorio_num = path +'/'+ nombre
          plt.savefig(directorio_num, format="pdf")
          plt.show()

          path_csv = path + '/' + 'data_clusters.csv'
          data.to_csv(path_csv,index=False)

        else:
          print('ALERTA: No se pueden clusterizar variables de una sola dimension')

############################################################################################
#Completa: Los detectores son: z_scores, rango_intercuartil, dbscan y isolation_forest

    def detect_anomalies(self, detector = 'z_scores',tecnica_limpieza = 0, tecnica_escalar = 0,variables_separar=[],k=0):
      #Creacion carpeta anomalies
      path = str(self.path) + '/anomalies'

      if os.path.exists(path) == False:
        os.mkdir(path)

      lista_detectores = ['z_scores','rango_intercuartil','dbscan','isolation_forest']

      data = self.df
      variables = data.columns
      var_text = variables.values.astype(str).tolist()

      if k != 0:
        self.df = data.drop(['Cluster'],axis=1)
        data = self.df
        variables = data.columns
        var_text = variables.values.astype(str).tolist()

      #-------------------------------------------------------------------------
      #Esto existe en el caso de que se aplique anomalies por si sola, dado
      # que cluster tambien aplica 4 y 5 no es necesario volver aplicarlo
      if k == 0:
        #----------------------------------------------------------------------
        #Se aplica 4
        var_text = variables.values.astype(str).tolist()
        clean_data_method = self.clean_data(var_text,metodo = tecnica_limpieza,eleccion_input= variables_separar).reset_index()
        self.df = clean_data_method.drop(['index'], axis=1)

        #Se reescribe la data
        data = self.df
        variables = data.columns
        var_text = variables.values.astype(str).tolist()
        #----------------------------------------------------------------------
        #Se aplica 5
        scale_method = self.scale(var_text,tecnicas= tecnica_escalar)
        #Se reescribe la data
        self.df = scale_method
        #----------------------------------------------------------------------
        data = self.df
        variables = data.columns
      #-------------------------------------------------------------------------

      # Seleccionar caracter√≠sticas num√©ricas y categ√≥ricas
      numeric_features = data[variables].select_dtypes(include=['number'])
      categorical_features = data[variables].select_dtypes(exclude=['number'])

      if len(numeric_features.columns) == 0:
        raise Exception('No se pueden detectar anomalias que no consideren valores numericos')
      #-----------------------------------------------------------------------------------------------
      #Tecnicas de deteccion de anomalias

      #Z_score
      if detector == 'z_scores':

        # Obtener los nombres de las columnas originales
        orig_column_names = numeric_features.columns.tolist()

        z_scores = zscore(numeric_features).abs() > 3

        # Crear un diccionario que mapea los nombres de las columnas originales a los nuevos nombres
        new_column_names = [f"{col}_anomalies" for col in orig_column_names]
        column_mapping = {orig: new for orig, new in zip(orig_column_names, new_column_names)}

        # Utilizar df.rename() con el diccionario de mapeo
        z_scores = z_scores.rename(columns=column_mapping)

        # Crear un DataFrame con los datos transformados
        z_score_df = pd.DataFrame(z_scores, columns=new_column_names)

        # A√±adir las columnas categ√≥ricas al DataFrame
        z_scores_df = pd.concat([data, z_score_df], axis=1)

        for k in range(len(new_column_names)):
            var = new_column_names[k]
            old_var = orig_column_names[k]

            # Dividir los datos en dos grupos seg√∫n el valor de var
            true_data = z_scores_df[z_scores_df[var] == True][old_var]
            false_data = z_scores_df[z_scores_df[var] == False][old_var]

            # Crear el histograma y asignar colores y leyendas
            plt.figure()
            plt.hist(true_data, color='blue', alpha=0.5, label='True')
            plt.hist(false_data, color='red', alpha=0.5, label='False')
            plt.xlabel(old_var)
            plt.ylabel('Frecuencia')
            plt.legend()

            nombre = f'Deteccion_de_anomalias_{k}.pdf'
            directorio_num = path + '/' + nombre

            # Guardar el gr√°fico en formato PDF
            plt.savefig(directorio_num, format="pdf")
            plt.show()

        path_csv = path + '/' + 'data_anomalies.csv'
        z_scores_df.to_csv(path_csv,index=False)

      #-----------------------------------------------------------------------------------------------
      #Rango intercuantil
      elif detector == 'rango_intercuartil':

        # Obtener los nombres de las columnas originales
        orig_column_names = numeric_features.columns.tolist()

        rango = numeric_features.describe()

        for name in orig_column_names:

          iqr = rango[name]["75%"] - rango[name]["25%"]
          cota_inf = rango[name]["25%"] - iqr * 1.5
          cota_sup = rango[name]["75%"] + iqr * 1.5
          column_names = str(name) + "_iqr_outlier"
          data[column_names] = (data[name] > cota_sup) | (data[name] < cota_inf)

          # Dividir los datos en dos grupos seg√∫n el valor de var
          true_data = data[data[column_names] == True]
          false_data = data[data[column_names] == False]

          # Crear el histograma y asignar colores y leyendas
          plt.figure()
          plt.hist(true_data[name], color='blue', alpha=0.5, label='True')
          plt.hist(false_data[name], color='red', alpha=0.5, label='False')
          plt.xlabel(name)
          plt.ylabel('Frecuencia')
          plt.legend()

          nombre = f'Deteccion_de_anomalias_iqr_{name}.pdf'
          directorio_num = path + '/' + nombre

          # Guardar el gr√°fico en formato PDF
          plt.savefig(directorio_num, format="pdf")
          plt.show()

        path_csv = path + '/' + 'data_anomalies.csv'
        data.to_csv(path_csv,index=False)
      #-----------------------------------------------------------------------------------------------
      #Estos metodos son multivariados, debemos separar con condicionalidad
      #Caso si posee mas de una variable numerica
      #Isolation forest
      elif detector == 'isolation_forest':

        if len(numeric_features.columns) > 1:
          if len(numeric_features) > 1000:
            print('*'*80)
            print('WARNING: Metodos que utilizan UMAP demoran mucho tiempo con esta cantidad de datos')
            print('*'*80)

          isf = IsolationForest(n_estimators=2)
          outliers = isf.fit_predict(numeric_features)

          umap_1 = umap.UMAP()

          a = umap_1.fit_transform(numeric_features)

          numeric_features["x_umap"] = a[:, 0]
          numeric_features["y_umap"] = a[:, 1]


          # Obtener los datos subyacentes del gr√°fico de Plotly
          x = numeric_features['x_umap']
          y = numeric_features['y_umap']

          plt.scatter(x, y, c= outliers == -1)

          # Configurar el t√≠tulo y las etiquetas de los ejes
          plt.title('Gr√°fico de dispersi√≥n UMAP')
          plt.xlabel('Eje X')
          plt.ylabel('Eje Y')

          nombre = 'Deteccion_de_anomalias_isf.pdf'
          directorio_num = path + '/' + nombre

          plt.savefig(directorio_num, format="pdf")
          plt.show()

          path_csv = path + '/' + 'data_anomalies.csv'

          categorical_feautures_f = categorical_features.reset_index()

          df_final = pd.concat([categorical_feautures_f, numeric_features], axis=1)
          df_final = df_final.drop(['index'],axis=1)

          df_final.to_csv(path_csv,index=False)

        else:
          raise Exception('Se esta intentando detectar una anomalia multivariada con datos de una sola dimension')

      #-----------------------------------------------------------------------------------------------
      #Dbscan
      elif detector == 'dbscan':

        if len(numeric_features.columns) > 1:
          if len(numeric_features) > 1000:
            print('*'*80)
            print('WARNING: Metodos que utilizan UMAP demoran mucho tiempo con esta cantidad de datos')
            print('*'*80)

          umap_1 = umap.UMAP()
          a = umap_1.fit_transform(numeric_features)
          numeric_features["x_umap"] = a[:, 0]
          numeric_features["y_umap"] = a[:, 1]

          clustering = DBSCAN(eps=0.3, min_samples=3).fit(numeric_features)

          db_scan_labels = clustering.labels_

          numeric_features[db_scan_labels == -1]

          # Crear un gr√°fico de dispersi√≥n
          plt.scatter(numeric_features['x_umap'], numeric_features['y_umap'], c=db_scan_labels == -1)

          # Configurar el t√≠tulo y las etiquetas de los ejes
          plt.title('Dispersion metodo DBSCAN')
          plt.xlabel('Eje X')
          plt.ylabel('Eje Y')

          nombre = 'Deteccion_de_anomalias_dbscan.pdf'
          directorio_num = path + '/' + nombre

          plt.savefig(directorio_num, format="pdf")
          plt.show()

          path_csv = path + '/' + 'data_anomalies.csv'

          categorical_feautures_f = categorical_features.reset_index()

          df_final = pd.concat([categorical_feautures_f, numeric_features], axis=1)

          df_final = df_final.drop(['index'],axis=1)

          df_final.to_csv(path_csv,index=False)

        else:
          raise Exception('Se esta intentando detectar una anomalia multivariada con datos de una sola dimension')

      elif detector not in lista_detectores:
        raise Exception('Ese metodo no esta considerado, intente con los siguientes: z_scores, rango_intercuartil, isolation_forest o dbscan')

############################################################################################
    def Profile(self):
      #Llamamos todos los metodos
      variables = self.df.columns

      #----------------------------------------------------------------------------------------------------------------------------
      #Summarize
      self.summarize(variables)
      print('x'*80)
      print('Se ejecuto: SUMMARIZE')
      #----------------------------------------------------------------------------------------------------------------------------
      #Clean_data
      print('x'*80)
      print('Tecnicas= drop, imputar_media y imputar_ceros')
      tecnica_limpieza_input = input('Si lo desea coloque el tipo de tecnica de limpieza de NaN que quiere utilizar (estandar es drop): ')

      print('-'*80)
      mensaje = 'Las variables DataFrame son: ' + str(variables.tolist()) + ' se debe colocar como lista ej: ["age", "Year"]'
      print(mensaje)
      variables_separar_input = input('Si lo desea coloque las variables no atomicas que quiere separar (estandar se separan todas las no-atomicas): ')
      print('-'*80)

      lista_tecnicas_limpieza = ['drop', 'imputar_media', 'imputar_ceros', 0]

      if tecnica_limpieza_input not in lista_tecnicas_limpieza:
        tecnica_limpieza_input = 0
        print('Tecnica de limpieza no identificada se procedera con drop')

      if len(variables_separar_input) != 0:
        variables_separar_input = eval(variables_separar_input)

      self.df = self.clean_data(variables.to_list(),metodo = tecnica_limpieza_input,eleccion_input = variables_separar_input)

      self.df.columns = self.df.columns.astype(str)
      self.variables = self.df.columns
      variables = self.df.columns

      print('x'*80)
      print('Se ejecuto: CLEAN_DATA')
      #----------------------------------------------------------------------------------------------------------------------------
      #Plot_vars
      print('x'*80)
      print(mensaje)
      top_categorias = input('Si lo desea coloque el top de categorias que desea graficar (si se omite se grafican todas): ')

      if len(top_categorias) != 0:
        top_categorias = eval(top_categorias)

      if not isinstance(top_categorias,list):
        top_categorias = []

      self.plot_vars(variables,top_n = top_categorias)
      print('Se ejecuto: PLOT_VARS')
      #----------------------------------------------------------------------------------------------------------------------------
      #Make clusters
      print('Tecnicas adicionales de escalamiento = StandardScaler y RobustScaler (recomendable no usar)')
      tecnica_escalamiento_input = input('Si lo desea coloque el tipo de tecnica de escalamiento (Orden: ADICIONAL - LOG - MINMAX): ')
      print('x'*80)


      lista_tecnicas_escalamiento = ['StandardScaler', 'RobustScaler']

      if tecnica_escalamiento_input not in lista_tecnicas_escalamiento:
        tecnica_escalamiento_input = 0
        print('Tecnica de escalamiento no identificada se procedera sin agregar tecnica adicional')


      self.make_clusters(tecnica_limpieza = tecnica_limpieza_input, tecnica_escalar = tecnica_escalamiento_input,variables_separar = variables_separar_input,k=1)
      self.df.columns = self.df.columns.astype(str)
      self.variables = self.df.columns
      variables = self.df.columns
      print('Se ejecuto: Make_Clusters')
      #----------------------------------------------------------------------------------------------------------------------------
      #Detect Anomalies
      print('x'*80)
      print('Los detectores son: z_scores, rango_intercuartil, dbscan y isolation_forest')
      detector_input = input('Si lo desea coloque el tipo de detector que quiere utilizar para las anomalias (estandar es z_score): ')
      print('x'*80)

      detector_input = str(detector_input)
      lista_detectores = ['z_scores', 'rango_intercuartil', 'dbscan', 'isolation_fores']

      if detector_input not in lista_detectores:
        detector_input = 'z_scores'
        print('Detector no identificado se procedera con z_scores')

      self.detect_anomalies(detector = detector_input,tecnica_limpieza = tecnica_limpieza_input, tecnica_escalar = tecnica_escalamiento_input,variables_separar = variables_separar_input,k=1)
      print('Se ejecuto: Detect_Anomalies')
      print('x'*80)
      #----------------------------------------------------------------------------------------------------------------------------

############################################################################################
    def clearGarbage (self):
      carpetas = ['anomalies','clean_data','clusters','plots','scale']
      path = str(self.path)

      #Se recorren las carpetas
      for i in carpetas:
        ruta = path + i

        try:
            # Borrar el contenido de las carpeta
            for elemento in os.listdir(ruta):
                elemento_ruta = os.path.join(ruta, elemento)
                if os.path.isfile(elemento_ruta):
                    os.remove(elemento_ruta)
                elif os.path.isdir(elemento_ruta):
                    shutil.rmtree(elemento_ruta)

            # Borrar las carpeta
            shutil.rmtree(ruta)

            print(f"Contenido de la carpeta y la carpeta '{ruta}' han sido borrados con √©xito.")
        except Exception as e:
            print(f"Se borro correctamente '{ruta}': {str(e)}")

      #Borrar summary y EDA
      ruta = path
      ruta_summary = path + '/summary'
      #Borrar summary
      os.remove(ruta_summary)

      # Borrar la carpeta EDA
      shutil.rmtree(ruta)

"""## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)

A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:

1. Introducci√≥n
    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.

Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.

Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:

- Describir la tarea asociada al dataset.
- Describir brevemente los datos de entrada que les provee el problema.
- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.

2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)
    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:
        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?
        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?
        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?
        - ¬øExisten datos duplicados en el conjunto?
        - ¬øExisten relaciones o patrones visuales entre las variables?
        - ¬øExisten anomal√≠as notables o preocupantes en los datos?
3. Creaci√≥n de Clusters y Anomal√≠as
    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.
    
4. An√°lisis de Resultados
    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.
5. Conclusi√≥n
    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos.
"""

import pandas as pd
olimpiadas=pd.read_parquet(r'/content/olimpiadas.parquet')

olimpiadas

#EDA sin clean data (archivo summarize)
n_columnas = olimpiadas.columns.to_list()

data_0 = Profiler(olimpiadas)
data_0.summarize(n_columnas)

#Para el EDA con la data limpia
new_df = data_0.clean_data(n_columnas, metodo = 'imputar_media', eleccion_input= ['age-height-weight'])
new_df

new_df[new_df['age-height-weight_2'] == 214]

#Nuevo summarize con datos limpios (se escribe debajo del anterior)
n_columnas = new_df.columns.to_list()

data_0_clean = Profiler(new_df)
data_0_clean.summarize(n_columnas)

"""### 1) Introducci√≥n


---
En medio de una crisis pol√≠tica y econ√≥mica en Chile, y en un entorno marcado por el resurgimiento de programas de televisi√≥n de baja calidad, todas las miradas y esperanzas se centran en un evento muy relevante: Los Juegos de Santiago 2023. El presidente, conocido como "Bomb√≠n," ha confiado a nuestro equipo el proyecto de llevar a cabo un an√°lisis exhaustivo de los datos relacionados con los Juegos Ol√≠mpicos, utilizando la clase "Profiler" que hemos previamente desarrollado.

Nuestro proyecto tiene como objetivo revitalizar al pa√≠s y brindar un tan necesario alivio a la sociedad a trav√©s de este evento deportivo. Para lograrlo, utilizaremos el conjunto de datos "olimpiadas.parquet," que contiene informaci√≥n sobre diversos Juegos Ol√≠mpicos celebrados en los √∫ltimos a√±os. Este conjunto de datos incluye columnas que describen aspectos como la identificaci√≥n del atleta, su nombre, g√©nero, equipo, c√≥digo del Comit√© Ol√≠mpico Nacional (NOC), a√±o de los Juegos Ol√≠mpicos, temporada de los juegos, ciudad anfitriona, deporte, evento, medallas ganadas y datos personales, como edad, altura y peso.

La tarea principal que abordaremos es realizar un an√°lisis integral de los datos de los Juegos Ol√≠mpicos, lo que implica la identificaci√≥n de informaci√≥n relevante sobre los atletas, los equipos, las disciplinas deportivas, las ciudades de los atletas y las medallas ganadas en diferentes ediciones de los Juegos Ol√≠mpicos.

Para llevar a cabo este an√°lisis, emplearemos la clase "Profiler" para realizar un an√°lisis exploratorio de datos, identificar estad√≠sticas descriptivas, visualizar distribuciones, explorar relaciones entre variables y detectar posibles anomal√≠as. Adem√°s, utilizaremos t√©cnicas de limpieza y escalado de datos, as√≠ como algoritmos de clustering y detecci√≥n de anomal√≠as, con el objetivo de obtener informaci√≥n valiosa a partir de los datos.

A lo largo de este proyecto, se presentar√°n los resultados de dicho an√°lisis sobre los datos de los Juegos Ol√≠mpicos, resaltando hallazgos interesantes y patrones significativos que contribuir√°n a una comprensi√≥n m√°s profunda de este conjunto de datos.

### 2) An√°lisis del EDA (An√°lisis Exploratorio de Datos)


---

¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?

De partida se presentan las siguientes variables num√©ricas y categ√≥ricas:

numericas = [ID,Year]

categoricas = [Name,Sex,Team,NOC,Games,Season,City,Sport,Event,Medal,age-height-weight]

En donde claramente existen m√°s variables categ√≥ricas que num√©ricas. De las num√©ricas el ID y Year no deber√≠a ser a priori datos que se manejen de forma num√©rica dado que no representan en si n√∫meros asociados a una m√©trica m√°s bien corresponden a informaci√≥n tabulada de forma num√©rica que representa alguna categor√≠a, de todos modos para el dataframe construido se estableci√≥ de dicha forma. Para el caso de las categ√≥ricas, tenemos informaci√≥n asociada a los competidores de los distintos a√±os en conjunto de las pruebas que participaron sumado a su nacionalidad, edad, ciudad en la que participaron, etc.

Referido a c√≥mo se comportan las variables tenemos lo siguiente para cada una:

ID = Corresponde a la ID de cada competidor siendo este √∫nico para cada uno de ellos. Este ID se mantiene independiente del a√±o que compitan, teniendo valores desde 1 hasta 135571 (sin saltarse ning√∫n valor).

Year = Corresponde al a√±o en que compite un atleta especifico, puede repetirse para distintos competidores dado que varios compitieron el mismo a√±o y posee valores desde 1896 hasta 2016.

Name = Nombre de cada competidor, se puede repetir dado que los competidores pueden competir en distintas pruebas y a√±os. El nombre m√°s repetido corresponde a 'Robert Tait McKenzie'. Los nombres pueden contener espacios al igual que s√≠mbolos como '(' y '-'.

Sex = Sexo del competidor, siendo M masculino y F Femenino (√∫nica dos opciones).

Team = Corresponde al equipo con el que participo el competidor, generalmente corresponde al pa√≠s.

NOC = Corresponde a las siglas del pa√≠s a que pertenece el competidor. Ej CHINA = CHN

Games = Corresponde al 'Juego' o competencia en la que se particip√≥. Es una mezcla de Year + Season, teniendo valores como 1992 Summer.

Season = Corresponde a la temporada en la que se realiz√≥ la competencia, siendo estas Summer (verano) y Winter (invierno).

City = Ciudad en la que se realiz√≥ la competencia, teniendo valores como Barcelona

Sport = Deporte en el que se particip√≥, ejemplo Basketball.

Event = Evento en el que se particip√≥, corresponde a una versi√≥n m√°s especifica del Sport/Deporte teniendo como por ejemplo Cycling Women's Road Race.

Medal = Corresponde a la medalla que obtuvo el competidor/a, siendo estas posibles [None, 'Gold', 'Bronze', 'Silver']. Solo se puede obtener una medalla por competencia y los competidores que no ganan nada tienen None.

age-height-weight = Es una variable asociada a la edad, tama√±o y peso, estos tres datos estan mezclados en la misma columna generando valores no-atomicos. Adem√°s dicha informaci√≥n puede presentar None como por ejemplo el siguiente valor '24.0(nan?nan'. Notar que dicha columna presenta separadores en sus distintos valores.

¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?

Existen valores nulos en una √∫nica columna siendo esta Medal con 231333 datos nulos, asociados a los competidores que no ganaron ninguna medalla en sus competencias. Adem√°s de esta, age-height-weight presenta valores nulos en sus datos no-atomicos pero no presenta ninguna NaN o None puro que se identifique como tal, por ello no se presenta ningun valor en su summary inicial.

¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?

Referido a la frecuencia de valores √∫nicos tenemos lo siguiente sobre las variables categoricas:

Name = se presentan 134732 lo que implica que se repiten valores del total dado que el dataframe posee 271116, esto tiene sentido dado que es normal que se repitan los competidores, pero ocurre que al analizar la data de forma directa no solo se repite el name sino que toda la fila existiendo datos repetidos en ese DataFrame. Bajo el contexto de este problema, no tiene sentido que existan datos repetidos dado que implicar√≠a que el competidor participo repetidas veces en la misma competencia el mismo dia compitiendo consigo mismo.

Team = Para esta variable existen 1184 valores √∫nicos los cuales implican que los teams no necesariamente representan pa√≠ses dado que solo existen 206 pa√≠ses en total, ejemplo de esto es el valor Australasia el cual representa un conjunto de pa√≠ses de Ocean√≠a en donde no necesariamente implica que dichos pa√≠ses participaron como equipo por separado otros a√±os. Del mismo modo se repiten valores para esta variable.

NOC = Referido a esta variable, se presentan 230 valores √∫nicos los cuales hacen referencia a los pa√≠ses existentes. El hecho de que existan mas que 206 hace referencia a los pa√≠ses que dejaron de existir y a los nuevos que se formaron como por ejemplo URS que hace referencia a la Union Sovietica. Relacion√°ndolo con la variable anterior el Team Soviet Union tambi√©n est√° inscrito.

Games = Esta variable presenta 51 datos √∫nicos los cuales se relacionan con el n√∫mero de valores √∫nicos de la columna Year, en donde games corresponde al doble de los a√±os presentes dado que cada a√±o presenta Winter y Summer referido al a√±o. Por ello deber√≠an haber 70 (35x2) valores pero existen a√±os que √∫nicamente tuvieron una solo Game como es el caso de 1900.

Season = Esta variable presenta solo dos valores como ya se expuso y no posee informaci√≥n relevante a su frecuencia. Algo notable de mencionar es que existen 222552 datos con Season == 'Summer' y solamente 48564 con Season == 'Winter', esto tiene sentido dado que es mas esperable tener dificultades en invierno que en verano.

City = Existen solo 42 ciudades en los que se han hecho juegos, lo cual proporciona informaci√≥n √∫nica dado que no puede ser desprendido de otra variable.

Sport = Para este caso existen 66 valores √∫nicos, lo cual implica que a lo largo de todos los a√±os los deportes en los que se han competido han sido pr√°cticamente los mismos.

Event = Esta variable posee 765 valores unicos lo cual es esperable dado que para cada deporte existen multiples eventos, siendo este n√∫mero mayor a 66. Si se hace la divisi√≥n existen aproximadamente 11 categor√≠as por juego lo cual a priori no es una m√©trica real dado que deben existir m√°s eventos para algunos juegos que para otros.

Medal = Referido a Medal solo existen tres valores √∫nicos siendo Bronzo, Plata y Oro, esto tiene sentido dado que nunca se han inventado categor√≠as extras para estos deportes.

age-height-weight = Esta variable posee 82971 valores √∫nicos lo cual es bastante interesante dado que como ya se coment√≥ los datos presentes en ella no parecieran tener ning√∫n sentido a priori, pero como se repiten datos es bastante posible que que tengan un orden establecido. Por ejemplo si analizamos un atleta aleatorio como 'Piotr ya' tenemos que su valor de esta variable corresponde a 27.0:176.0?59.0 y 27.0*176.0?59.0 siendo que ambas filas poseen la misma informaci√≥n (es el mismo juego) pero los separadores son distintos, lo mismo pasa si analizamos la informaci√≥n del atleta que mas se repite 'Robert Tait McKenzie' en donde el participo en mas de 10 juegos teniendo en todos para esta variable la informaci√≥n de esta maneta 'age:nan?nan' siendo age el numero correcto asociado a su edad (se actualizo con los a√±os). Por ello, por mucho de que pareciera que la informaci√≥n de esta columna no tiene sentido si mantiene un orden asociado a cada atleta, que es necesario almacenar. Aun asi, pareciera ser que el tama√±o y el peso son informaciones que no se almacenan para todos los juegos, lo cual convierte dichas variables en un gran problema asociada a la cantidad de valores faltantes que se generan.

¬øExisten datos duplicados en el conjunto?

Como ya se expuso existen datos repetidos en todas las variables donde inclusive se mantienen los mismos valores para age-height-weight, esto es un alivio dado que se puede solucionar con una limpieza de datos. En el caso en donde age-height-weight no mantuviera un orden con los elemntos separadores de sus valores no atomicas, generarian dificultades al momento de filtrar las filas dado que esta variable no tendrian los mismos valores. Basicamente se destaca que existe una concordancia entre los atletas y los valores de age-height-weight.

¬øExisten relaciones o patrones visuales entre las variables?

S√≠, solo por el contexto del problema las variables se relacionan como por ejemplo Season con Games, Year con Games, etc. Exponiendo todas estas relaciones en el an√°lisis previo. Una relaci√≥n interesante es entre age y Year en donde se podria sacar la diferencia de edad viendo las distintas participaciones de lxs atletas.

¬øExisten anomal√≠as notables o preocupantes en los datos?

S√≠, la mayor anomal√≠a est√° asociada a la variable age-height-weight en donde como ya se expuso es un alivio que sea concordante con cada atleta pero el hecho de que exista informaci√≥n faltante para alguna de las 3 (espec√≠ficamente height y weight) provoca que la separaci√≥n de esta varible tienda a generar muchos datos faltantes, lo que provoca que el an√°lisis de las posibles nuevas columnas sea bastante deplorable dada la poca cantidad de datos que tendr√≠an. Para el caso de Medal el hecho de que presente muchos datos faltantes no es un problema dado que se pueden reemplazar por una nueva categor√≠a como 'No gano', esto proporcionar√≠a m√°s informaci√≥n para el problema.

### 2) Con la data limpia


---

Al aplicar clean data obtenemos el segundo dataframe de arriba, en ese caso los datos repetidos fueron eliminados y la variable age-height-weight fue separada en tres nuevas columnas con sufijos 0 hasta 2, representando la informaci√≥n asociada a age, height y weight respectivamente. La t√©cnica utilizada para trabajar los missing values fue la de imputar medias dado que estos en su mayor√≠a corresponde a la variable Medal (lo que tiene sentido), por ello aplicar drop genera perdida de informaci√≥n quedando dos opciones imputar medio o imputar ceros, en donde la segunda opci√≥n no tiene sentido en el contexto del problema debido a que a√±adir un 0 establece que existe un atleta con 0 a√±os, 0 peso o 0 altura, por ello se decidi√≥ por la primera. Cabe destacar que aplicar la media a las variables at√≥micas nuevas implica que la informaci√≥n faltante asociada a la edad, tama√±o y peso de lxs atletas es en promedio la misma, esto a priori no pareciera ser una apuesta descabellada dado que los atletas por lo general presentan las mismas contexturas f√≠sicas.

El nuevo dataframe presenta 269731 datos, por lo cual se eliminaron 1385 datos repetidos los cuales no tienen sentido en el contexto del problema. Para las tres nuevas variables num√©ricas se tiene que age (age-height-weight_0) posee 75 valores √∫nicos de los cuales el m√°ximo valor corresponde 97 para el atleta 'John Quincy Adams Ward' que participo en escultura, por el otro lado el menor valor corresponde a 10 que corresponde a 'Dimitrios Loundras' en gimnasia, ambos resultados concordantes con la informaci√≥n de internet. Para dicha variable se tiene que el promedio de age corresponde 25 a√±os y presenta un gran numero de outlayers 11059 sin ser suficientes para levantar la Alerta del m√©todo summarize (10% outlayers).

Referido a la variable height (age-height-weight_1), se tiene que el mayor valor corresponde a 226 lo que se traduce en 2,26 metros para el/la atleta 'Yao Ming', dicho valor es concordante con la informacion de internet (es el que sale en los simpsons). Para esta variable el promedio corresponde 175 (1,75 metros) y posee 10529 outlayers, lo cual no es suficiente para levantar la alerta del summary. Cabe destacar que esta variable presenta 96 valores unicos.

Finalmente, la variable weigth (age-height-weight_2) referida al peso de lxs atletas tiene 221 valores unicos, siendo el mayor 214 y el menor 25. Para el caso del maximo dicha informacion corresponde a 'Ricardo Blas, Jr' que corresponde a un atleta de judo el cual efectivamente pesa eso en kilos. Referido al promedio se tiene que el peso de lxs atletas corresponde a 70,7 kilos y existen 12294 outlayers sin ser suficientes para levantar la alerta de la funci√≥n.

Por lo tanto, el comportamiento general de los nuevos datos pareciera estar en orden, teniendo que la informaci√≥n presente en ellos concuerda con la referida a internet, el hecho de que los valores maximos y minimos tengan sentido dentro del contexto del problema exponen que los outlayers tienen l√≥gica, del mismo modo demuestran un comportamiento correcto de estas variables. Adem√°s, si solo analizamos los promedios de estas variables nos encontramos con un o una atleta que posee 25 a√±os, de estatura 1,75 y peso 70 kilos, lo cual es concordante con la realidad.

### 3) Creaci√≥n de Clusters y Anomal√≠as

---



La creaci√≥n de clusters y la detecci√≥n de anomal√≠as son etapas cruciales en nuestro an√°lisis de datos de los Juegos Ol√≠mpicos. La elecci√≥n de los algoritmos y sus hiperpar√°metros se basa en criterios espec√≠ficos para garantizar la efectividad de estas tareas.

En el caso de la creaci√≥n de clusters, optamos por el algoritmo K-Means debido a su versatilidad en cuanto al n√∫mero de clusters, lo que nos permite adaptar el an√°lisis a la estructura latente de los datos. Para determinar el n√∫mero √≥ptimo de clusters, utilizamos el m√©todo del codo previamente calculado. Este m√©todo consiste en evaluar la inercia de los clusters para diferentes valores de k y seleccionar aquel valor de k donde la inercia comienza a estabilizarse. Esto garantiza que los clusters sean representativos y significativos en funci√≥n de la estructura de los datos.

En lo que respecta a la detecci√≥n de anomal√≠as, consideramos varios m√©todos: 'z_scores', 'rango_intercuartil', 'DBSCAN' y 'Isolation Forest'. La elecci√≥n de m√∫ltiples m√©todos se basa en la importancia de abordar la detecci√≥n de anomal√≠as desde diferentes perspectivas. 'Z-scores' y 'rango intercuartil' son m√©todos estad√≠sticos tradicionales que se utilizan para identificar valores at√≠picos en funci√≥n de la distribuci√≥n de los datos. Por otro lado, 'DBSCAN' es un algoritmo de clustering que tambi√©n se puede utilizar para detectar anomal√≠as en grupos de baja densidad, y 'Isolation Forest' es un algoritmo que se enfoca en aislar instancias an√≥malas, es decir, outliers.

### 4) An√°lisis de Resultados


---

El an√°lisis de resultados es importante para evaluar la efectividad de las t√©cnicas aplicadas en la manipulaci√≥n y procesamiento de datos. Para dicho proyecto, se han implementado diversas etapas que permiten una mejor comprensi√≥n de los datos y su posterior separaci√≥n en cl√∫steres, as√≠ como la identificaci√≥n de anomal√≠as.

En primer lugar, se ha aplicado el proceso "Clear Data", que implica la limpieza y preprocesamiento de los datos. Este paso es importante debido a las caracterizaciones del dataset, por lo que se realiza para eliminar valores at√≠picos, datos faltantes y ruido en los datos. Despu√©s, se ha aplicado la separaci√≥n de "datos no-at√≥micos", lo que facilita la manipulaci√≥n y an√°lisis individual de cada elemento del conjunto junto con la entrega de nueva informaci√≥n.

Se puede ver que no se encontraron datos con valores nulos gracias al m√©todo seleccionado para la limpieza de datos, en concreto, para la separaci√≥n de datos relacionados con la edad, altura y peso.

Como parte del an√°lisis exploratorio de datos, se han generado gr√°ficos de densidad para las variables num√©ricas. Estos gr√°ficos proporcionan una representaci√≥n visual de la distribuci√≥n de los datos despu√©s de la separaci√≥n, lo que puede revelar patrones y tendencias. Con ello, se ha realizado una matriz de correlaci√≥n para las variables num√©ricas, lo que permite identificar relaciones entre ellas. Este an√°lisis es importante para entender c√≥mo las variables est√°n interconectadas y si existen dependencias que pueden influir en la separaci√≥n de datos.

Al observar los gr√°ficos se pueden ver distribuciones de edad, altura, peso y a√±os, junto con diferentes correlaciones entre ellas. Referido al grafico de Year se tiene que la mayor cantidad de datos esta presente entre los a√±os 1970 hasta 2010, lo cual tiene logica debido a que todos los a√±os se fomenta mas el deporte habiendo mas atletas y deportes como por ejemplo skate. Para los edades, alturas y pesos se tienen gaussianas casi perfectas centrando sus valores en 25, 175 y 70 respectivamente, esto tiene logica debido a que dichas variables son efectivamente gaussianas y se asocian con la tecnica de imputacion de datos aplicada en el clean_data. Sobre la matriz de correlacion se tiene que los datos presentan bajos valores, siendo su mayor valor entre height ('age-height-weight_1') y weight ('age-height-weight_2') lo cual es normal debido a que las personas mas altas tienden a pesar mas.

Adem√°s, se ha examinado un histograma de las principales categor√≠as junto con la matriz de Cramer correspondiente. Esto ayuda a comprender la distribuci√≥n de las categor√≠as y su impacto en el an√°lisis de datos. En caso de requerir un enfoque diferente en el an√°lisis de variables categ√≥ricas, se contempla la posibilidad de utilizar la t√©cnica de "one-hot encoding," aunque se advierte que debido a la estructura del conjunto de datos, esto podr√≠a generar una cantidad excesiva de columnas, lo que dificultar√≠a la visualizaci√≥n debido al gran tama√±o del gr√°fico. Las categorias escogidas corresponden a ['NOC','Sex','Medal','Sport'] dado que nos interesaba saber si existian relacion entre dichas variables.

Al observar los gr√°ficos se pueden ver la distribuci√≥n de las top n categor√≠as junto con su matriz de cramer, referido a los histogramas nos encontramos con que efectivamente existen nacinalidades (NOC) que han participida mas veces en estos eventos que otras, existiendo una gran diferencia entre algunas. Sobre la variable Sex, se tiene que han participado muchos mas hombres (M) que mujeres (F), habiendo una diferencia de casi el doble, esto nuevamente tiene sentido dado el contexto historico de los deportes. Sobre la variable Medal se tiene que el numero de medallas de bronce, playa y oro es basicamente la misma, lo cual tiene sentido debido a que si se da una medalla de oro tambien se da de las otras, habiendo una consistencia en los registros, existiendo en la mayoria 'None' referido a los competidores que no ganaron ninguna medalla. Finalmente referido a Sport, se tiene que hay deportes mas frecuentes que otros, nuevamente esto tiene sentido debido a que hay mas categorias para ciertos deportes que para otros, del mismo modo que historicamente no se han practicado los mismos deportes. Referido a la matriz de Cramer, se tiene que las categorias no comparten mucha relacion solo haiendo un dato destacable que el de Sex con Sport, en donde comparten un valor de 0.32 (32%) que es logico debido a que ciertos deportes solo deben tener categoria de hombres dado el contexto historico (o mas frecuencia para esos a√±os), en si el hecho de que participen mas hombres que mujeres coloca una relacion a la variable Sex.

Tambi√©n, se ha aplicado el m√©todo Log Min-Max, que ayuda a escalar los datos y normalizarlos, lo que es esencial para asegurar una correcta comparaci√≥n y an√°lisis.  Luego, se tiene la opcion de aplicar el m√©todo Scale para realizar un escalado de las variables utilizando StandardScaler y RobustScaler antes del Log Min-Max, dado el comportamiento de la data se opto por no utilizarlos. El escalado es esencial para asegurar que todas las variables tengan el mismo peso en el an√°lisis y generar mejores resultados en la clusterizacion.

Se ha utilizado el m√©todo del codo para determinar el n√∫mero √≥ptimo de cl√∫steres. Esta t√©cnica implica la representaci√≥n gr√°fica de la inercia en funci√≥n del n√∫mero de cl√∫steres, y el n√∫mero √≥ptimo se selecciona cuando la inercia deja de disminuir significativamente. Esto proporciona una base s√≥lida para la separaci√≥n de datos en cl√∫steres mediante el algoritmo K-Means.
En el gr√°fico del m√©todo del "Codo," se seleccion√≥ el n√∫mero de cl√∫steres "10" que muestra una distribuci√≥n particular. Aunque la separaci√≥n de datos resulta efectiva, se destaca que esta no es muy evidente debido a la escasa correlaci√≥n entre las variables, como se evidenci√≥ en la matriz de correlaci√≥n previamente mencionada. Un conjunto de datos de mayor calidad y con correlaciones m√°s pronunciadas permitir√≠a una representaci√≥n m√°s clara de la separaci√≥n entre los cl√∫steres. Ademas de esto, se destaca que la existencia de variables como ID generan malos resultados dado que no proporcionan informacion del problema, por esto mismo se realizo otra clusterizacion sin esta columna en las celdas de mas abajo.

Finalmente, se ha aplicado el m√©todo Detect Anomalies para identificar anomal√≠as en el conjunto de datos. Se han empleado diversas t√©cnicas, como Z-scores, rango intercuart√≠l, DBSCAN e Isolation Forest, lo que permite una detecci√≥n exhaustiva de datos at√≠picos o an√≥malos en el conjunto de datos.
En este caso, se opt√≥ por utilizar Z-scores debido a la falta de correlaci√≥n significativa entre las variables, lo que hace que t√©cnicas como DBSCAN o Isolation Forest sean menos convenientes (los clusters no tienen mucha logica). Dado que las medidas son de naturaleza f√≠sica y los datos presentan densidades considerables, los Z-scores resultan una mejor opci√≥n, ya que son un m√©todo estad√≠stico tradicional para identificar valores at√≠picos en funci√≥n de la distribuci√≥n de los datos. Las anomal√≠as detectadas indican que los datos que se alejan tres desviaciones est√°ndar del promedio son considerados outliers, como se refleja en los histogramas ("True" coloreados). Los resultados arrojan que existen pocos outlayers similar al analisis EDA de los datos limpios, basicamente el escalado y la metrica Z-score apuntan que los datos presentan pocos outlayers.

### Primer ejecucion con ID
"""

data_1 = Profiler(olimpiadas)
data_1.Profile()

data_1.clearGarbage()

"""### Segunda ejecucion sin ID"""

data_sin_id = olimpiadas.drop(['ID'],axis=1)

data_sin_id

data_2 = Profiler(data_sin_id)
data_2.Profile()

data_2.clearGarbage()

"""### Tercera ejecucion sin ID ni Year"""

data_sin_id_year = data_sin_id.drop(['Year'],axis=1)

data_sin_id_year

data_3 = Profiler(data_sin_id_year)
data_3.Profile()

data_3.clearGarbage()

"""### 4.1) An√°lisis de Resultados


---
Los distintos experimentos que se realizaron extrayendo las variables de ID y Year arrojaron resultados un poco mejores, pero en la practica la cauterizaci√≥n no es buena dado que los clusters est√°n muy pegados entre ellos. Si se relaciona dicho resultado con el contexto del problema se tiene que existen 11 tipos de deportistas/atletas/competidores en donde cada subgrupo presenta una que otra caracter√≠stica similar al resto, siendo estas el peso, altura o edad. Esto tiene sentido dado que si bien cada deportista posee un cuerpo espec√≠fico para su disciplina la edad o el tama√±o por lo general siguen siendo par√°metros similares entre ellas, por ello no se pueden separar los subgrupos. Adem√°s, referido al n√∫mero, 11 tipos de deportistas no parece descabellado dado que existen muchos deportes distintos en donde las variables analizadas varian en gran medida o espec√≠ficamente en una, por ejemplo, las personas que participan en basketball por lo general son j√≥venes y altos, cosa que es similar para los que practican judo pero pueden variar dr√°sticamente en peso debido a que este √∫ltimo deporte tiene categor√≠a de +100 kilos, esto generar√≠a que ambos clusters est√©n pegados pero desplazados en sentidos opuestos de una dimensi√≥n que es el caso para mucho de los clusters obtenidos, sin tener que ser espec√≠ficamente el peso la variable en la que difieren.

### 5) Conclusi√≥n


---
En conclusi√≥n, se tiene que el Profiler funciono de forma adecuada permitiendo generar data limpia y legible. De las conclusiones que se extrajeron fueron que el m√©todo permite generar columnas atomicas de forma correcta, permitiendo obtener informaci√≥n relevante para el problema. Adem√°s de los histogramas se obtuvo que existe relaci√≥n entre la variable peso y altura, al igual que existen deportes m√°s frecuentes que otros habiendo m√°s participaci√≥n hist√≥rica de hombres que mujeres. Referido a la cauterizaci√≥n se obtuvo que no se formaron grupos tan llamativos ni separados y se asoci√≥ dicho resultado con que en general los deportistas siempre comparten caracter√≠sticas en alg√∫n aspecto, adem√°s se destac√≥ que las variables num√©ricas eran pocas y en las mayor√≠as de los casos no se correlacionaban como era para el caso de la variable ID y Year.

Finalmente se vuelve a destacar en todo el an√°lisis que las t√©cnicas implementadas al igual que las decisiones tomadas, son relevantes para generar resultados buenos, dado que un incorrecto manejo de ellas puede provocar perdida de informaci√≥n al igual que conclusiones sin sentido.

### 6) gifs

---

#### Buscando el pixel de variable numerica

![Gracias Totales!](https://i.pinimg.com/originals/db/db/ba/dbdbbad5798bfc3ff27a499dc5ca2b30.gif)

#### Como que ID y Year son las unicas variables numericas?

![Gracias Totales!](https://tenor.com/es-419/view/shinji-screaming-gif-21090977.gif)

#### Como que un ni√±o de 10 a√±os participo en los juegos olimpicos?

![Gracias Totales!](https://media.tenor.com/BZHMyUHUOT8AAAAd/musculos-ni%C3%B1o.gif)

![Gracias Totales!](https://media.tenor.com/D5numtVEMfIAAAAC/baby-strong.gif)
"""